{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Microsoft Outlook\n",
    "\n",
    "Do you spend a lot of time sorting and filtering your mails in Outlook?\n",
    "Have you enough to create filtering rules for each new type of mail and modify them each time a mail is a little different that what you specified?\n",
    "If yes, this tutorial is for you!\n",
    "\n",
    "I had the same problem, receiving several hundreds to thousands of mail each day, and creating endless rules that slowed down my outlook a lot. I then developed a script in Python using sklearn (a machine learning library for Python) to automatically sort my mails.\n",
    "\n",
    "Indeed, machine learning applies perfectly in this case, because :\n",
    "- I have already a lot of mails (lots of data)\n",
    "- These mails are already well sorted\n",
    "- They don't follow extremely simple rules (such as \"move all the mails containing 'order' in the folder 'Order') but rather more complex rules that will take some times to define and which are prone to be modified with future mails (such as \"move all the mails containing \"order\" from this person to this other person with these other keyword A,B,...,Z and without this keyword 1,2,... from this date to this date..)\n",
    "\n",
    "\n",
    "The code I will introduce below contains two main parts :\n",
    "- The trainer : part that will train the model using all the folders specified (*) with the emails inside\n",
    "- The predictor : part that will predict the category of a new received mail\n",
    "\n",
    "I use in the model a random forest (RF) classifier which, after experimentations, is in this case the fastest and efficient model to classify the mails.\n",
    "For information, I tried to use neural networks (MLP with Keras) for this task but it was much more time and CPU consumming for no or very little gain. Better use the best tools for a specific task !\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf8\n",
    "\n",
    "import win32com.client #win32 allows to access microsoft applications like outlook\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier #our model with use a random forest\n",
    "from sklearn.feature_extraction.text import CountVectorizer #we will need to normalise our input data before feeding it to our model\n",
    "from sklearn.externals import joblib #we will need to save our model after the training to retrieve it before the prediction\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "import pytz\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from bs4 import BeautifulSoup #contains a method to extract raw text from html data\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import re #regular expressions\n",
    "\n",
    "import sys\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "Note that above we import nltk.corpus\n",
    "If you haven't already installed the Natural Language Toolkit or one of the corpus needed, you will have to do the following :\n",
    "- pip install nltk\n",
    "- python\n",
    "- import nltk\n",
    "- nltk.download()\n",
    "- Select \"Corpora/Stopwords\" and download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the global definitions :\n",
    "X is the input vector\n",
    "y is output vector\n",
    "outlook_folders will list all the folders containing the intelligent_folder_identifier (by default, the folders containing '.'\n",
    "path_file_vectorizer and path_file_classifier are the paths where we will save the vectorize and the classifier\n",
    "nb_clf_estimators is the number used for the random forest (1000 is a good value, not too big, not too small)\n",
    "nb_max_minutes_to_classify_mail is the number maximum of ancienty for a mail used when we retrieve the mails for predictions, -1 to classify all the mails not already classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#start global definitions\n",
    "outlook = win32com.client.Dispatch(\"Outlook.Application\").GetNamespace(\"MAPI\")\n",
    "inbox = outlook.GetDefaultFolder(6) #6= Inbox\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "outlook_folders=[]\n",
    "\n",
    "#you might want to modify the following variables\n",
    "path_file_vectorizer='data//vectorizer.pkl'\n",
    "path_file_classifier='data//clf.pkl'\n",
    "intelligent_folder_identifier=\".\" #We define \".\" as the identifier to identify the 'smart' folders\n",
    "nb_clf_estimators=1000 #Number of estimators used in our model (random forest classifier)\n",
    "nb_max_minutes_to_classify_mail=-1 #We will each classify each mail older than less than nb_max_minutes_to_classify_mail in each folder\n",
    "#end global definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'get_relevant_info_from_mail' keeps only essential informations about the email, here :\n",
    "the sender name, the sender email address, the recipient, the message, the CC, the BCC, the subject and the body.\n",
    "I don't store the dates but that would be interesting if a certain order in the mail will change the category. In this case a LSTM neural network might be more efficient..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_relevant_info_from_mail(msg):\n",
    "\t#This function keeps only the relevant info from the mail\n",
    "\ttry:\n",
    "\t\treturn msg.SenderName+\" \"+msg.SenderEmailAddress+msg.To+msg.CC+msg.BCC+msg.Subject+msg.Body\n",
    "\texcept Exception as e:\n",
    "\t\tprint(\"Error in get_relevant_info_from_mail\")\n",
    "\t\ttry:\n",
    "\t\t\tprint('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(e).__name__, e)\n",
    "\t\texcept:\n",
    "\t\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'strip_text' will get the output of 'get_relevant_info_from_mail' and strips it from all undesirable data like html tags, non-letters characters, remove the stop words and finally get a cleaned string with only real words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_text(raw_text):\n",
    "\t# This function strips raw text from all html tags, accentuated characters, remove stop words and keep only meaningful words\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_text,\"lxml\").get_text() \n",
    "    # 2. Remove non-letters       \n",
    "    accentedCharacters = \"àèìòùÀÈÌÒÙáéíóúýÁÉÍÓÚÝâêîôûÂÊÎÔÛãñõÃÑÕäëïöüÿÄËÏÖÜŸçÇßØøÅåÆæœ\" \n",
    "    letters_only = re.sub(\"[^a-zA-Z\"+accentedCharacters+\"\\.]\", \" \", review_text) \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    # 4. In Python, searching a set is much faster than searching a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    # 6. Join the words back into one string separated by space, and return the result.\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function train_model is the most interesting part.\n",
    "X is the input dataset. Each input of X is a string containing only real words which is the output of strip_text.\n",
    "But for a random forest, it is not exploitable as such.\n",
    "You will need to convert each of these strings in an array of 0s and 1s.\n",
    "The method to do that is to use the CountVectorizer.\n",
    "\n",
    "In summary, the CountVectorizer will find all the words used in each of the strings of X and create a global dictionnary with these words. Each string of X wil then be compared to the global dictionnary and convert to an array of 0s an 1s such as this example :\n",
    "Consider a very simple dictionnary : [\"Machine\",\"Learning\",\"is\",\"very\",\"great\",\"awesome\",\"good\"]\n",
    "and a string 1: \"Machine Learning is awesome\"\n",
    "and a string 2: \"Learning is great\"\n",
    "You will then have the string 1 the following array : [1,1,1,0,0,1,0]\n",
    "and for the string 2 : [0,1,1,0,1,0,0]\n",
    "\n",
    "Two more things to be noted about CountVectorizer :\n",
    "max_df = 0.95\n",
    "min_df = 0.005\n",
    "\n",
    "These two parameters allows us to reduce the dimensionality of our X matrix by using TF-IDF to identify un-important words. \n",
    "\n",
    "The min_df paramter makes sure we exclude words that only occur very rarely\n",
    "The default also is to exclude any words that occur in every string (like 'the' ,'is', 'are'...)\n",
    "\n",
    "We are excluding all words that occur in too many or too few documents, as these are very unlikely to be discriminative. Words that only occur in one document most probably are names, and words that occur in nearly all documents are probably stop words.\n",
    "\n",
    "\n",
    "Now that we have a normalized X (X1) and y, which is our output dataset containing the identifiant of the category (0 for the first category, 1 for the second category, 2 for the third category...), we can train the model\n",
    "\n",
    "The function returns the vectorizer, the classifier and the score. The latter is not useful in the training but can be used for benchmarking or following the score over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(nb_clf_estimators):\n",
    "\t# The min_df paramter makes sure we exclude words that only occur very rarely\n",
    "\t# The default also is to exclude any words that occur in every movie description\n",
    "\t# We are excluding all words that occur in too many or too few documents, as these are very unlikely to be discriminative\n",
    "\t# More about these parameters on : https://spandan-madan.github.io/\n",
    "\tvectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 5000,max_df=0.95, min_df=0.005)\n",
    "\tX1=vectorizer.fit_transform(X)\n",
    "\n",
    "\t# Here we train train a random forest because it is quick to train and works very well for text classification\n",
    "\tclf = RandomForestClassifier(n_estimators = nb_clf_estimators) \n",
    "\tclf = clf.fit(X1, y)\n",
    "\tscore=clf.score(X1, y)\n",
    "\n",
    "\treturn [vectorizer,clf,score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'set_dataset' retrieves the relevant informations from each mail from each folders passed as parameters and adds for each mail a new line in X (relevant string) and y (category).\n",
    "Note that we only deal with message with class=43 because events, notes don't have necessarily the same fields that the standard messages and need a specific treatment (not implemented in this tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_dataset(folder,value):\n",
    "\tfor msg in reversed(folder.Items):\n",
    "\t\ttry:\n",
    "\t\t\tif msg.Class==43: #We only want to keep pure mails (this exclude appointments and notes for example)\n",
    "\t\t\t\ttxt0=get_relevant_info_from_mail(msg)\n",
    "\t\t\t\tif txt0!=None:\n",
    "\t\t\t\t\ttxt=strip_text(txt0)\n",
    "\t\t\t\t\tX.append(txt)\n",
    "\t\t\t\t\ty.append(value)\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(\"error when adding this mail to our dataset\",msg.Subject)\n",
    "\t\t\ttry:\n",
    "\t\t\t\tprint('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(e).__name__, e)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'predict_mail_category' is used to predict the category and move accordingly all the mails inside the folder passed as parameters.\n",
    "The function takes into input the vectorizer and the classifier and the folders (array that contains all the smart folders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_mail_category(vectorizer,clf,folders,folder):\n",
    "\tdt=pytz.utc.localize(datetime.utcnow())+timedelta(minutes=-nb_max_minutes_to_classify_mail)\n",
    "\n",
    "\tfor msg in reversed(folder.Items):\n",
    "\t\ttry:\n",
    "\t\t\tif msg.Class==43:\n",
    "\t\t\t\tif msg.sentOn>dt or nb_max_minutes_to_classify_mail==-1:\n",
    "\t\t\t\t\ttxt0=get_relevant_info_from_mail(msg)\n",
    "\t\t\t\t\tif txt0!=None:\n",
    "\t\t\t\t\t\ttxt=strip_text(txt0)\n",
    "\t\t\t\t\t\tX1=vectorizer.transform([txt])\n",
    "\t\t\t\t\t\tprediction=clf.predict(X1)\n",
    "\t\t\t\t\t\tmsg.Move(folders[prediction[0]])\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(\"error when adding this mail to our dataset\",msg.Subject)\n",
    "\t\t\ttry:\n",
    "\t\t\t\tprint('Error on line {}'.format(sys.exc_info()[-1].tb_lineno), type(e).__name__, e)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'loop_through_folder' iterates recursively through all the smart folders and smart subfolders in the inbox and lauching the function action passed as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_through_folder(k,folders,action):\n",
    "#Recursive function that iterate through folder and subfolder and launch the action method if there is a \".\" in the folder name\n",
    "\tfor folder in folders:\n",
    "\t\tif intelligent_folder_identifier in folder.Name:\n",
    "\t\t\t# print(\"We will add the mails of this folder in our dataset\",folder.Name,\"with y=\",k)\n",
    "\t\t\taction(folder,k)\n",
    "\t\t\tk=k+1\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tl=len(folder.Folders)\n",
    "\t\t\tloop_through_folder(k,folder.Folders,action)\n",
    "\t\texcept:\n",
    "\t\t\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'main_train_model_from_folders' is the main function to train the random forest.\n",
    "It will display the total score and time taken to train the model\n",
    "and then will save the vectorizer and the classifier for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_train_model_from_folders():\n",
    "\tprint(\"Start training\")\n",
    "\tstart = timer()\n",
    "\tloop_through_folder(0,inbox.Folders,set_dataset)\n",
    "\t[vectorizer,clf,score]=train_model(nb_clf_estimators)\n",
    "\tend = timer()\n",
    "\n",
    "\tprint(\"Score=\",score*100,\"%\")\n",
    "\tprint(\"Total time taken to train the model:\",end-start,\"s\")\n",
    "\n",
    "\t#Dumps vectorizer and classifier (random forest classifier) in files such as to be retrieve later when we want to classify a mail\n",
    "\tjoblib.dump(vectorizer, path_file_vectorizer)\n",
    "\tjoblib.dump(clf, path_file_classifier)\n",
    "\n",
    "\tprint(\"End training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'main_predict_category_for_each_mail' is the function used to move all the mails according the predicted categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main_predict_category_for_each_mail():\n",
    "\tprint(\"Start prediction\")\n",
    "\t#Loads vectorizer and classifier previously stored after having run the main_fetch_mails_and_train_model function\n",
    "\tstart = timer()\n",
    "\tvectorizer=joblib.load(path_file_vectorizer) \n",
    "\tclf=joblib.load(path_file_classifier) \n",
    "\n",
    "\tloop_through_folder(0,inbox.Folders,lambda folder,k: outlook_folders.append(folder))\n",
    "\n",
    "\tpredict_mail_category(vectorizer,clf,outlook_folders,inbox)\n",
    "\tend = timer()\n",
    "\n",
    "\tprint(\"End prediction\")\n",
    "\tprint(\"Total time taken to classify the mails:\",end-start,\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing the program ouside jupyter, you can specify if you want to train or predict.\n",
    "To do so :\n",
    "- To train : python outlook_train_and_predict.py train\n",
    "- To predict : python outlook_train_and_predict.py predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown keyword, use 'train' to train the model and 'predict' to predict the category of each mail\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tif len(sys.argv)!=2:\n",
    "\t\taction=\"\"\n",
    "\telse:\n",
    "\t\taction=sys.argv[1]\n",
    "\n",
    "\tif action==\"train\":\n",
    "\t\tmain_train_model_from_folders()\n",
    "\telif action==\"predict\":\n",
    "\t\tmain_predict_category_for_each_mail()\n",
    "\telse:\n",
    "\t\tprint(\"Unknown keyword, use 'train' to train the model and 'predict' to predict the category of each mail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Score= 100.0 %\n",
      "Total time taken to train the model: 90.29 s\n",
      "End training\n"
     ]
    }
   ],
   "source": [
    "main_train_model_from_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction\n",
      "End prediction\n",
      "Total time taken to classify the mails: 3.038297298569887 s\n"
     ]
    }
   ],
   "source": [
    "main_predict_category_for_each_mail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Author\n",
    "\n",
    "Axel de Raismes\n",
    "\n",
    "- https://www.linkedin.com/in/axelderaismes/\n",
    "- http://www.thegeeklegacy.com\n",
    "- https://twitter.com/axelderaismes\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
